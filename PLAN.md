# NextXO Employee Monitor - Development Plan & History

## Session: November 24, 2025

### Overview
Initial setup and rapid development of the NextXO Employee Monitor dashboard, focusing on AI training capabilities and branding.

### Key Decisions & Implementations

#### 1. Project Architecture
- **Stack**: React + Vite + Tailwind CSS + Shadcn/UI.
- **Backend Decision**: Added a lightweight Node.js/Express server (`server.js`) running on port 3000.
  - *Reasoning*: We needed a way to save annotated datasets permanently. Browser-only storage is insufficient for large image datasets. The server handles file uploads and unzipping.

#### 2. Style & UI
- **Style Guide**: Established `Style_instruction.md` early on.
- **Theme**: Professional dark mode with "Primary Blue" accents to convey trust and security.
- **Components**: utilized Shadcn/UI for rapid, accessible component development.

#### 3. AI Training Module (`/training`)
- **Workflow**: Users can upload images -> Annotate them -> Save to server -> Train model.
- **Annotation Tool**: Built `ImageAnnotator.tsx` from scratch.
  - *Decision*: Uses `JSZip` to bundle images and JSON annotations on the client side. This ensures atomic uploads and keeps the dataset organized.
- **Training Simulation**: Integrated `@tensorflow/tfjs`.
  - *Current State*: The training process is currently a simulation (visual progress bars and state updates) to demonstrate the UX flow. The structure is ready for actual tensor operations.
  - *Logic Fix*: Implemented logic to handle "Catastrophic Forgetting". When training new classes, we explicitly mark unselected classes as `isTrained: false` to reflect that the model would need retraining on the full dataset in a real scenario.
- **Persistence**: Used `localStorage` to persist class names and counts across reloads.

#### 4. Rebranding (Lovable -> NextXO)
- **Identity**: Renamed project to "NextXO Employee Monitor".
- **Logo**: Created a custom SVG logo (Shield + Circuit design) to represent "AI Security".
- **Cleanup**: Removed `lovable-tagger` and related metadata from `index.html`, `package.json`, and `vite.config.ts` to ensure a clean codebase.

### Future Roadmap / Next Steps
- **Real AI Training**: Connect the TensorFlow.js simulation to actual `model.fit()` calls using the uploaded datasets.
- **Python Integration**: The `dataset/` folder is structured to be consumed by the Python scripts in `training_scripts/` for more powerful offline training.
- **Live Inference**: Connect the `Monitor.tsx` page to a real video feed and run the TF.js model for real-time detection.
- **KNN Movement Training**: Currently uses a local webcam for training movement/gestures. 
  - *Future Task*: Replace local webcam access with a connection to the IP camera stream for the production environment.

## Session: November 25, 2025

### Achievements
- **Work Instruction Builder (`BuildWI.tsx`)**:
  - Implemented a drag-and-drop interface for creating assembly sequences.
  - Added routine scheduling (Frequency & Time).
  - Enabled saving of Work Instructions to `localStorage`.
- **Team Management (`ManageTeam.tsx`)**:
  - Created a new page to manage teams and assign Work Instructions.
  - Implemented member management (Add/Remove employees).
  - Linked team assignments to the Monitor view.
- **Monitor Page Enhancements (`Monitor.tsx`)**:
  - **Dynamic Task Loading**: Tasks now automatically populate based on the employee's team assignment.
  - **AI Integration**: Integrated MediaPipe for hand tracking and prepared the pipeline for custom object detection.
  - **Interactive Workflow**: Added a "Start Task" sequence with a countdown timer.
  - **Karaoke-Style Verification**: Implemented a speech-to-text system where spoken words highlight in real-time to verify step completion.
- **Infrastructure**:
  - Created `.env` file for IP Camera configuration.
  - Updated navigation and routing to include new modules.

### Current Status
- The system now supports a full lifecycle: **Train AI -> Build Instructions -> Assign to Team -> Monitor Execution**.
- The Monitor page features a sophisticated interactive overlay for guiding employees through tasks.

### Next Steps
- **Backend Integration**: Move from `localStorage` to a real database for Teams and Work Instructions.
- **Real Model Inference**: Connect the custom object detection model to the Monitor page's visualization.
- **Reporting**: Implement the "Export Report" functionality in Quick Actions.

---

## Session: November 25, 2025 (Evening) - Gesture Recognition Overhaul

### Major Changes: GRU ‚Üí DTW + k-NN

#### Problem with Previous Approach
- The GRU (neural network) approach required significant training time and data
- Velocity-based gesture detection was unreliable (timeout issues, coordinate bugs)
- Complex recording triggers made UX frustrating

#### New Architecture: Dynamic Time Warping (DTW) + k-Nearest Neighbors
- **DTW Algorithm**: Measures similarity between temporal sequences regardless of speed variations
  - Perfect for gestures performed at different speeds
  - Custom implementation with Sakoe-Chiba band for O(n¬≤) ‚Üí O(n¬∑w) optimization
- **k-NN (k=3)**: Simple majority vote classification from nearest templates
  - No training required - just stores reference sequences
  - Instant "training" by building template library
- **Normalization**: Landmarks normalized to wrist origin, scaled by palm size

#### Key Implementation Files
- `gesture_workflow/scripts/dtw_gesture.py`: Main DTW + k-NN classifier
  - Modes: `--train`, `--stream`, `--classify`, `--info`
  - Uses pickle (.pkl) for model serialization
- `src/pages/Training.tsx`: Updated training UI

#### Recording System Overhaul
- **Removed**: Velocity-based motion detection (was buggy)
- **Added**: Fixed-duration recording per gesture class
- **Per-Gesture Duration**: Each gesture class stores its own recording duration (2-6 seconds)
  - Set when creating the gesture class in "Add Gesture" dialog
  - Duration selector: 2s, 3s, 4s, 5s, 6s buttons
- **Test Mode**: Uses fixed 3-second duration for universal testing

#### UI Updates
- **Add Gesture Dialog**: Now includes duration selector
- **Webcam Overlay**: Shows selected gesture's recording duration
- **Record Button**: Displays duration (e.g., "Record 3s")
- **Badge System**: "Trained" vs "New" status for each gesture class

#### Data Structure
```json
// gesture_workflow/classes.json
{
  "classes": [{
    "id": "gesture_xxx",
    "name": "wave",
    "displayName": "Wave",
    "duration": 3,
    "createdAt": "..."
  }]
}
```

#### Server Updates (`server.js`)
- POST `/api/gestures/classes` now accepts `duration` field
- Model endpoints updated for DTW (.pkl files instead of .h5)
- Sequence storage unchanged (JSON files per recording)

### Benefits of DTW Approach
1. **No Training Wait**: Templates stored instantly
2. **Speed Invariant**: Same gesture at different speeds still matches
3. **Interpretable**: Can debug by examining distance to each template
4. **Low Data Requirement**: Works with just 1-3 samples per class

### Current Status
- ‚úÖ DTW implementation complete
- ‚úÖ Per-gesture duration feature complete
- ‚úÖ Recording system simplified
- ‚úÖ Server updated for new model format
- üîÑ Ready for testing with real gestures

---

## Session: November 26, 2025 - DTW Performance Optimization

### Problem Identified
During real-world testing, gesture classification was taking **too long** (500-1000ms per request), making the system feel unresponsive for real-time use.

### Root Cause Analysis
1. **Process Spawn Overhead (~200-300ms)**: Every classification request spawned a new Python process
   - Python interpreter startup time
   - Loading all imports (numpy, etc.)
   - Loading the pickle model from disk
   
2. **Unoptimized DTW Computation (~27ms for 12 templates)**:
   - Full 50-frame sequences being compared
   - Comparing against ALL training samples (12 templates)
   - O(n √ó m √ó num_templates) complexity

### Solutions Implemented

#### 1. Sequence Downsampling
```python
def downsample_sequence(seq, target_frames=20):
    """Downsample to fixed frame count using linear interpolation"""
    indices = np.linspace(0, len(seq) - 1, target_frames).astype(int)
    return seq[indices]
```
- Reduces frames from ~50 to 20 (60% reduction)
- Preserves gesture shape through index-based sampling

#### 2. Centroid Templates
```python
def compute_centroid(sequences):
    """Average multiple templates into one representative"""
    # Resample all to same length, then average
```
- Instead of comparing against all 12 samples, compute 1 centroid per class
- Reduces comparisons from O(num_samples) to O(num_classes)
- 2 classes = 2 centroids vs 12 templates = **6x fewer comparisons**

#### 3. Persistent Python Process
```javascript
// server.js - Keep inference process alive
let gestureInferenceProcess = null;

function startGestureProcess() {
    gestureInferenceProcess = spawn(PYTHON_PATH, [SCRIPT, '--stream']);
    // Keeps running, accepts multiple requests
}
```
- Model loaded once on server start
- No process spawn overhead per request
- Auto-reloads after training completes

### Performance Results

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| DTW computation | ~27ms | ~1ms | **27x faster** |
| API response time | 500-1000ms | 12-22ms | **40-80x faster** |
| Process startup | Every request | Once | **Eliminated** |

### Files Modified
- `gesture_workflow/scripts/dtw_gesture.py`:
  - Added `downsample_sequence()` function
  - Added `compute_centroid()` function
  - Updated `DTWGestureClassifier` with centroid support
  - Modified `save()` to build centroids before saving
  - Modified `load()` to restore centroids
  
- `server.js`:
  - Added persistent `gestureInferenceProcess`
  - Added `startGestureProcess()` function
  - Added `/api/gestures/reload` endpoint
  - Modified `/api/gestures/classify` to use persistent process
  - Auto-reload process after training completes

### Accuracy Verification
- Leave-one-out cross-validation: **100%** accuracy maintained

---

## Session: November 26, 2025 (Afternoon) - Live Monitor Enhancement

### Goals
Enhance the Monitor page with comprehensive task verification:

1. **Task Requirements Panel** (below webcam)
   - Show required gesture with visual indicator
   - Show required spoken phrase
   - Show required component to present to camera
   
2. **Speech-to-Text Integration**
   - Use Web Speech API for real-time speech recognition
   - Compare spoken text against step description
   - Fuzzy matching for natural language variations
   
3. **Triple Verification System**
   - ‚úã Gesture verification (DTW classifier)
   - üé§ Speech verification (speech-to-text matching)
   - üì¶ Component verification (YOLO detection)

### Implementation Status

#### ‚úÖ Phase 1: Enhanced Task Requirements UI
- Added "Requirements" panel below webcam showing:
  - Current step number and description
  - Required gesture with real-time verification status
  - Required component with detection status
  - Required speech phrase with listening controls
  - Overall progress bar

#### ‚úÖ Phase 2: Speech-to-Text
- Integrated Web Speech API (`webkitSpeechRecognition`)
- Added `speechPhrase` field to WIStep interface
- Implemented fuzzy string matching with Levenshtein distance
- Visual feedback during listening (microphone button states)
- 70% match threshold for verification

#### ‚úÖ Phase 3: Integration
- All three verification methods working together
- Individual verification states tracked separately
- Progress bar shows combined verification status
- States reset properly between steps

### Files Modified
- `src/pages/Monitor.tsx`:
  - Added speech recognition state and refs
  - Added `toggleListening()` function
  - Added `fuzzyMatch()` and `levenshteinDistance()` functions
  - Updated `checkStepVerification()` to include speech
  - Added Task Requirements Panel UI
  - Updated reset logic in step transitions
  
- `src/pages/BuildWI.tsx`:
  - Added `speechPhrase` field to `WIStep` interface
  - Added speech phrase input in step builder UI
  - Added Mic icon and helper text

### Current Status
- ‚úÖ Task Requirements Panel implemented
- ‚úÖ Speech-to-Text working with fuzzy matching
- ‚úÖ Triple verification system complete
- ‚úÖ All verification states properly managed
- üîÑ Ready for testing

---

## Session: November 26, 2025 (Evening) - Speech Recognition Debug

### Problem
The "Test Speech" button wasn't detecting any words. User couldn't see what the website was hearing.

### Root Cause
1. **Stale Closure Issue**: The `useEffect` that initialized speech recognition had multiple dependencies (`isListening`, `spokenText`, etc.) causing it to recreate the recognition object frequently
2. **Callback State Capture**: The `onresult` and `onend` callbacks captured stale state values
3. **No Interim Results Display**: Only final (confirmed) text was shown - no real-time feedback

### Solutions Implemented

#### 1. Single Initialization Pattern
```tsx
// OLD - recreated on every state change
useEffect(() => { ... }, [isListening, spokenText, karaokeWords, ...]);

// NEW - initialize once on mount
useEffect(() => { ... }, []); // Empty deps
```

#### 2. Ref-Based State Tracking
```tsx
const isListeningRef = useRef(false); // For callbacks
const [isListening, setIsListening] = useState(false); // For UI

// Use ref in callbacks (always current value)
recognition.onend = () => {
  if (isListeningRef.current) {
    recognition.start(); // Auto-restart
  }
};
```

#### 3. Interim + Final Text Display
```tsx
const [spokenText, setSpokenText] = useState(""); // Final confirmed
const [interimText, setInterimText] = useState(""); // Real-time

// In onresult:
setInterimText(interimTranscript); // Shows immediately (yellow)
if (finalTranscript) {
  setSpokenText(prev => prev + finalTranscript); // Confirmed (white)
}
```

#### 4. Enhanced Test UI
- **Yellow text**: Real-time interim words (what's being processed)
- **White text**: Final confirmed words
- **Red pulsing dot**: Recording indicator
- Better error handling with toast notifications

### Files Modified
- `src/pages/Monitor.tsx`:
  - Added `interimText` state
  - Added `isListeningRef` for callback state tracking
  - Rewrote speech recognition initialization (empty deps)
  - Updated `toggleListening()` to manage ref
  - Enhanced Test Speech overlay with interim display

### Testing Instructions
1. Start dev server: `npm run dev`
2. Go to Monitor page for emp-001
3. Click "Test Speech" button (top-right of camera)
4. Speak into microphone
5. Yellow text = what's being heard in real-time
6. White text = confirmed words

### Current Status
- ‚úÖ Speech recognition properly initialized once
- ‚úÖ Ref-based state for reliable callbacks
- ‚úÖ Real-time interim text display
- ‚úÖ Clear visual feedback
- üîÑ Ready for testing

---

## Session: November 26, 2025 (Night) - Verification Flow Overhaul

### Problems Identified

#### 1. Test Gesture Button Missing
- No quick way to test gesture detection outside of verification flow
- Had to start a full task just to test if gestures were working

#### 2. Frame Format Mismatch
- Monitor.tsx was sending flat arrays: `[[x1,y1], [x2,y2], ...]`
- Training.tsx and DTW API expected: `{timestamp, left_hand: {landmarks: [[]]}, right_hand: {landmarks: [[]]}}`
- Caused classification to fail silently

#### 3. Detection Loop Issues
- Only drew 5 fingertip keypoints (Training.tsx draws all 21)
- Skipped hand detection during test gesture
- Continuous API calls during verification (spamming server)

#### 4. Gesture Hallucination (Multiple API Calls)
- Detection loop runs at ~30fps
- Condition `elapsed >= GESTURE_RECORD_DURATION` was checked every frame
- React state updates (`setIsRecordingGesture(false)`) are async
- Multiple frames triggered classification before state updated = "thousands of detections"

### Solutions Implemented

#### 1. Added Test Gesture Button
```tsx
// Top-right of webcam
<Button onClick={startTestGesture}>üñêÔ∏è Test Gesture</Button>
```
- 3-2-1 countdown
- 3-second recording with "Show your hand..." wait phase
- Single DTW API call
- Auto-dismiss result after 3 seconds

#### 2. Fixed Frame Format
```tsx
// extractHandLandmarks() - matches Training.tsx exactly
const extractHandLandmarks = (hand) => {
  const wrist = hand.keypoints[0];
  const landmarks = hand.keypoints.map(kp => [
    (kp.x - wrist.x) / 100,  // Normalized to wrist
    (kp.y - wrist.y) / 100
  ]);
  return { landmarks };
};

// Frame format
{
  timestamp: elapsed,
  left_hand: { landmarks: [[x,y], ...] } | null,
  right_hand: { landmarks: [[x,y], ...] } | null
}
```

#### 3. Simplified Detection Loop
- Always detect hands (removed conditional skipping)
- Always update `lastHandsRef` for async access
- Draw all 21 keypoints like Training.tsx
- Red dots during test, green dots normally

#### 4. Record-Then-Classify Approach (Verification)
**Old Flow (Bad)**:
- Continuous detection throughout verification
- Multiple API calls at random intervals
- Unreliable results

**New Flow (Fixed)**:
1. Countdown 3-2-1
2. **Recording Phase** (2 seconds): Collect frames, show "RECORDING GESTURE..." overlay
3. **Classify Once**: Single API call with all collected frames
4. **Speech Phase**: Show gesture result, prompt for speech
5. **Final Summary**: PASS/FAIL with details

#### 5. Ref-Based Classification Lock
```tsx
const isClassifyingGestureRef = useRef(false);

// In detection loop:
if (elapsed >= GESTURE_RECORD_DURATION && !isClassifyingGestureRef.current) {
  isClassifyingGestureRef.current = true; // LOCK - prevents duplicate calls
  const frames = [...gestureFramesRef.current];
  gestureFramesRef.current = []; // Clear immediately
  
  classifyGestureDTW(frames).then(result => {
    // Handle result once
  });
}

// Reset on new verification:
isClassifyingGestureRef.current = false;
```

### Visual Indicators Added
- **Red pulsing overlay**: "RECORDING GESTURE... 2s" during recording phase
- **Green/Yellow banner**: Shows detected gesture after classification
- **Animated badges**: Gesture/Component/Speech status at bottom

### Constants Updated
- `GESTURE_RECORD_DURATION = 2000` (2 seconds)
- `VERIFICATION_DURATION = 8000` (8 seconds total)
- `LOCK_CONFIDENCE = 0.5` (50% threshold)

### Files Modified
- `src/pages/Monitor.tsx`:
  - Added Test Gesture functionality
  - Added `extractHandLandmarks()` function
  - Added `lastHandsRef` for async hand access
  - Added `isClassifyingGestureRef` to prevent multiple API calls
  - Added recording phase states (`isRecordingGesture`, `gestureRecordingComplete`)
  - Updated detection loop for record-then-classify
  - Added visual overlays for recording phase
  - Draw all 21 keypoints instead of 5
  
- `server.js`:
  - Added detailed logging for classify requests
  - Logs frame count, first frame structure, result with all probabilities

### Current Status
- ‚úÖ Test Gesture button working
- ‚úÖ Frame format matches Training.tsx
- ‚úÖ All 21 keypoints drawn
- ‚úÖ Single API call per verification (ref lock)
- ‚úÖ Visual recording indicator
- ‚úÖ Auto-dismiss test results after 3s

---

## Session: November 27, 2025 - Verification Flow Issues

### Problem Summary
The step verification flow is not working reliably:
- Test Gesture button works correctly (detects gesture, shows result)
- But Start Step Verification often fails even when gesture is correct
- Shows "STEP FAILED" at timeout even when gesture was detected and matched
- Complex state management making debugging difficult

### What Works ‚úÖ
1. **Test Gesture Button** - Records 3s, classifies via DTW, shows result
2. **DTW Classification** - Server responds correctly (~20ms)
3. **Hand Detection** - MediaPipe detects hands reliably
4. **Speech Recognition** - Web Speech API works, verifies correctly

### What's Broken ‚ùå
1. **Step Verification** - Doesn't reliably complete when gesture matches
2. **State Synchronization** - `lockedGesture` not being set properly in all code paths
3. **Timeout Logic** - Shows "FAILED" even when gesture was detected

### Attempted Fixes (Nov 27)
1. Added `softResetVerification()` to keep verified items on retry
2. Added auto-verify `useEffect` that watches `currentGesture`
3. Simplified `performGestureVerification()` to match Test Gesture logic
4. Removed "Hold still" complexity

### Current Status
- Code committed but verification still unreliable
- May need to **redesign the verification flow from scratch**

### Root Cause Analysis
The verification system has grown too complex with multiple overlapping mechanisms:
- `checkStepVerification()` - old callback-based approach
- `performGestureVerification()` - async recording approach
- Auto-verify `useEffect` - watches `currentGesture`
- Timeout `useEffect` - checks `lockedGesture`

These don't coordinate well together, leading to race conditions and missed state updates.

### Recommendation: Simplify Architecture
**Option A: Single Source of Truth**
- Use ONLY the Test Gesture approach for verification
- Remove `checkStepVerification()` callback
- Remove complex timeout logic
- Just: Record ‚Üí Classify ‚Üí Match ‚Üí Pass/Fail

**Option B: Complete Redesign**
- Start fresh with a simpler state machine
- States: IDLE ‚Üí COUNTDOWN ‚Üí RECORDING ‚Üí CLASSIFYING ‚Üí RESULT
- Single function handles all transitions
- Clear success/failure paths

---

## Next Steps (Updated Priority)

### 1. üî¥ **REDESIGN Verification Flow** (CRITICAL)
- Choose Option A or B above
- Implement clean state machine
- Test thoroughly before adding features

### 2. üü° Component Detection Integration
- After verification is stable

### 3. üü¢ Database Integration
- After core features work

### 4. üîµ Reporting
- After data persistence

---

## Quick Reference

### Start Development
```bash
cd /home/noclout/employee-monitor-view
npm run dev      # Vite (port 8080)
node server.js   # Backend (port 3000)
```

### Test Gesture
1. Open Monitor page: http://localhost:8080/monitor/emp-001
2. Click "üñêÔ∏è Test Gesture" button (top-right of webcam)
3. Wait for 3-2-1 countdown
4. Show gesture for 3 seconds
5. See result with confidence %

### Key Files
- `src/pages/Monitor.tsx` - Live monitoring with verification
- `src/pages/Training.tsx` - Gesture training UI
- `gesture_workflow/scripts/dtw_gesture.py` - DTW classifier
- `server.js` - Backend API



